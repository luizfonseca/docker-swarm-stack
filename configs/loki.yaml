# https://grafana.com/docs/loki/latest/configure/#common_config
auth_enabled: false

server:
  http_listen_port: 3100
  grpc_server_max_recv_msg_size: 50485760 # 50MB

common:
  path_prefix: /etc/loki
  storage:
    filesystem:
      chunks_directory: /etc/loki/chunks
      rules_directory: /etc/loki/rules
  replication_factor: 1
  ring:
    instance_addr: 127.0.0.1
    kvstore:
      store: inmemory

frontend:
  address: 0.0.0.0
  max_outstanding_per_tenant: 1024
  max_body_size: 50485760 # 50MB

# https://grafana.com/docs/loki/latest/operations/storage/schema/
schema_config:
  configs:
    - from: 2023-10-01
      store: tsdb
      object_store: filesystem
      schema: v12
      index:
        prefix: index_
        period: 24h

storage_config:
  tsdb_shipper:
    active_index_directory: /etc/loki/tsdb-index
    cache_location: /etc/loki/tsdb-cache
    shared_store: filesystem
  filesystem:
    directory: /etc/loki/chunks

chunk_store_config:
  chunk_cache_config:
    async_cache_write_back_buffer_size: 1
    default_validity: 5m
    fifocache:
      ttl: 5m
      size: 0
      max_size_bytes: 256MB

# https://grafana.com/docs/loki/latest/operations/storage/retention/
compactor:
  working_directory: /etc/loki/retention
  shared_store: filesystem
  compaction_interval: 5m
  retention_enabled: true
  retention_delete_delay: 2h
  retention_delete_worker_count: 50

limits_config:
  retention_period: 768h # 32 days
  per_stream_rate_limit: 10MB
  ingestion_rate_mb: 80
  ingestion_burst_size_mb: 140
  # retention_stream:
  #   - selector: '{agent="vector"}'
  #     priority: 1
  #     period: 24h

table_manager:
  retention_deletes_enabled: true
  retention_period: 336h # 14 days

ruler:
  alertmanager_url: http://localhost:9093

querier:
  # Each `querier` component process runs a number of parallel workers to process queries simultaneously.
  # You may want to adjust this up or down depending on your resource usage
  # (more available cpu and memory can tolerate higher values and vice versa),
  # but we find the most success running at around `16` with tsdb
  max_concurrent: 3
